{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4325ff-71ae-46f4-aacc-a3107fb0c083",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Race to low rms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2945fbc-d119-407f-88e5-47731e7318fc",
   "metadata": {},
   "source": [
    "Import latin-hypercube test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0d5cdc-cd9e-4e66-b4e2-c70a66e3af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import healpy as hp\n",
    "import utils_intensity_map as uim\n",
    "import utils_deck_generation as idg\n",
    "import netcdf_read_write as nrw\n",
    "import training_data_generation as tdg\n",
    "import tf_neural_network as tfnn\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import utils_optimizers as uopt\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "run_dir = \"Data_input\"\n",
    "\n",
    "filename_flipped_trainingdata = \"flipped_training_data_and_labels.nc\"\n",
    "imap_nside = 256\n",
    "LMAX = 30\n",
    "hemisphere_symmetric = True\n",
    "run_clean = True\n",
    "rng = np.random.default_rng(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9d5b9-619b-4c93-ac41-fbf22450ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_params = tdg.define_system_params(run_dir)\n",
    "sys_params[\"trainingdata_filename\"] = filename_flipped_trainingdata\n",
    "X_all, Y_all, avg_powers_all = nrw.import_training_data(sys_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4473ad-556f-45e6-abe0-9fdf24d4aaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_all = np.sqrt(np.sum(Y_all**2, axis=0))\n",
    "\n",
    "target_mean = np.mean(target_all)\n",
    "target_variance = np.sqrt(np.var(target_all))\n",
    "print(target_mean*100.0, target_variance*100.0)\n",
    "\n",
    "input_means = np.mean(X_all, axis=1)\n",
    "input_standard_deviation = np.sqrt(np.var(X_all, axis=1))\n",
    "print(input_means*100.0, input_standard_deviation*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6857d4-5f18-4461-84d6-fb9a09e5dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(X_all[:,0]), np.shape(Y_all[:,0]))\n",
    "mindex = np.argmin(np.mean(Y_all, axis=0))\n",
    "print(mindex)\n",
    "print(np.sum(Y_all[:,mindex]))\n",
    "print(target_all[mindex])\n",
    "mindex = np.argmin(target_all)\n",
    "print(mindex)\n",
    "print(np.sum(Y_all[:,mindex]))\n",
    "print(target_all[mindex])\n",
    "\n",
    "num_examples = np.shape(X_all)[1]\n",
    "print(num_examples)\n",
    "num_inputs = np.shape(X_all)[0]\n",
    "num_modes = np.shape(Y_all)[0]\n",
    "print(num_inputs, num_modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c95c5-32a6-4cf8-aeb5-a1956c662522",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "plt.plot(np.arange(LMAX), Y_all[:,mindex] * 100.0)\n",
    "ax.set_xticks(range(0, LMAX+1, int(LMAX/5)))\n",
    "plt.xlim([0, LMAX])\n",
    "plt.title(\"Unweighted Modes\")\n",
    "plt.xlabel(\"l mode\")\n",
    "plt.ylabel(r\"amplitude ($\\%$)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff8ef3-69fb-42b2-b223-f827dd5d65b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 1, Brute force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1ab928-25b7-40d7-9491-4fc4e821de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e8c427-27ad-4e0e-bd90-70d47385ab61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b01aa36-7861-4579-8661-aef3dcba1048",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 2, Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c2f2f7-6611-4457-b6b0-ce4ca6f64853",
   "metadata": {},
   "source": [
    "The partial derivative is determined using a 2*16=32 grid of points (2 points in every dimension) around the current minima. These points can be evaluated in either a NN or Ifriit depending on speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de2225-af14-4b2a-9d77-2f51f29b775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_exp = -1.0\n",
    "n_iter = 10\n",
    "run_dir = \"Data_output\"\n",
    "iter_dir = \"iter_\"\n",
    "filename_flipped_trainingdata = \"flipped_training_data_and_labels.nc\"\n",
    "num_parallel = 17\n",
    "stencil_size = num_inputs * 2 + 1\n",
    "num_steps_per_iter = num_parallel - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a90c871-3f77-44cb-a496-e5be04d7a1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 10.0**learn_exp\n",
    "step_size = np.array([learn_exp - 1.0, learn_exp + 1.0])\n",
    "\n",
    "X_old = np.zeros((num_inputs, 1))\n",
    "Y_old = np.zeros((num_modes, 1))\n",
    "avg_powers_old = np.array([0.0])\n",
    "\n",
    "mindex = np.argmin(np.sqrt(np.sum(Y_all**2, axis=0)))\n",
    "X_old[:,0] = X_all[:, mindex]\n",
    "\n",
    "pbounds = np.zeros((num_inputs, 2))\n",
    "pbounds[:,1] = 1.0\n",
    "tic = time.perf_counter()\n",
    "for ieval in range(n_iter):\n",
    "    \n",
    "    if (sum(abs(X_all[:,-1] - X_all[:,-2])) <= 0.0):\n",
    "        learn_exp = learn_exp-0.5\n",
    "        learning_rate = 10.0**(learn_exp)\n",
    "        step_size = step_size - 0.5\n",
    "        print(\"Reducing step size to: \" + str(learning_rate))\n",
    "        if learning_rate < 1.0e-4:\n",
    "            print(str(ieval+1) + \" Bayesian data points added, saving to .nc\")\n",
    "            print(\"Early stopping due to repeated results\")\n",
    "            filename_trainingdata = run_dir + '/' + filename_flipped_trainingdata\n",
    "            nrw.save_training_data(X_all, Y_all, avg_powers_all, filename_trainingdata)\n",
    "            break\n",
    "    \n",
    "    X_stencil = uopt.gradient_stencil(X_old, learning_rate, pbounds, num_inputs, stencil_size)\n",
    "    Y_stencil, avg_powers_stencil = tdg.run_ifriit_input(stencil_size, X_stencil, run_dir, LMAX, num_parallel, hemisphere_symmetric, run_clean)\n",
    "    target_stencil = np.sqrt(np.sum(Y_stencil**2, axis=0))\n",
    "    mindex_stencil = np.argmin(target_stencil)\n",
    "    print(\"The minimum in the stencil\", np.min(target_stencil), mindex_stencil)\n",
    "    print(\"The previous value was: \", target_stencil[0], 0)\n",
    "    print(X_stencil[:,0])\n",
    "    os.rename(run_dir + \"/run_\" + str(mindex_stencil), run_dir + \"/\" + iter_dir + str(ieval+num_examples))\n",
    "    \n",
    "    grad = uopt.determine_gradient(X_stencil, target_stencil, learning_rate, pbounds, num_inputs)\n",
    "    X_new = uopt.grad_descent(X_old, grad, step_size, pbounds, num_inputs, num_steps_per_iter)\n",
    "    \n",
    "    Y_new, avg_powers_new = tdg.run_ifriit_input(num_steps_per_iter, X_new, run_dir, LMAX, num_parallel, hemisphere_symmetric, run_clean)\n",
    "    target_downhill = np.sqrt(np.sum(Y_new**2, axis=0))\n",
    "    mindex_downhill = np.argmin(target_downhill)\n",
    "    print(\"The minimum downhill\", np.min(target_downhill), mindex_downhill)\n",
    "    \n",
    "    if target_downhill[mindex_downhill] < target_stencil[mindex_stencil]:\n",
    "        shutil.rmtree(run_dir + \"/\" + iter_dir + str(ieval+num_examples))\n",
    "        os.rename(run_dir + \"/run_\" + str(mindex_downhill), run_dir + \"/\" + iter_dir + str(ieval+num_examples))\n",
    "        X_old[:,0] = X_new[:,mindex_downhill]\n",
    "        Y_old[:,0] = Y_new[:,mindex_downhill]\n",
    "        avg_powers_old = avg_powers_new[mindex_downhill]\n",
    "    else:\n",
    "        X_old[:,0] = X_stencil[:,mindex_stencil]\n",
    "        Y_old[:,0] = Y_stencil[:,mindex_stencil]\n",
    "        avg_powers_old = avg_powers_stencil[mindex_stencil]\n",
    "    \n",
    "    X_all = np.hstack((X_all, X_old))\n",
    "    Y_all = np.hstack((Y_all, Y_old))\n",
    "    avg_powers_all = np.hstack((avg_powers_all, avg_powers_old))\n",
    "    \n",
    "    print(\"Iteration {} with learn rate {} value:{}\".format(ieval, learning_rate, np.sqrt(np.sum(Y_old**2))))\n",
    "    print(X_old[:,0])\n",
    "    \n",
    "    if (np.sqrt(np.sum(Y_all[:,-1]**2)) > np.sqrt(np.sum(Y_all[:,-2]**2))):\n",
    "        print(\"Bug! Ascending slope!\")\n",
    "        print(np.sqrt(np.sum(Y_all[:,-1]**2)), np.sqrt(np.sum(Y_all[:,-2]**2)))\n",
    "        break\n",
    "    \n",
    "    if (ieval+1)%10 <= 0.0:\n",
    "        toc = time.perf_counter()\n",
    "        print(\"{:0.4f} seconds\".format(toc - tic))\n",
    "        print(str(ieval+1) + \" Bayesian data points added, saving to .nc\")\n",
    "        filename_trainingdata = run_dir + '/' + filename_flipped_trainingdata\n",
    "        nrw.save_training_data(X_all, Y_all, avg_powers_all, filename_trainingdata)\n",
    "        mindex = np.argmin(np.mean(Y_all, axis=0))\n",
    "        print(mindex)\n",
    "        print(np.sum(Y_all[:,mindex]))\n",
    "        print(np.sqrt(np.sum(Y_all[:,mindex]**2)))\n",
    "        mindex = np.argmin(np.sqrt(np.sum(Y_all**2, axis=0)))\n",
    "        print(mindex)\n",
    "        print(np.sum(Y_all[:,mindex]))\n",
    "        print(np.sqrt(np.sum(Y_all[:,mindex]**2)))\n",
    "for isten in range(stencil_size):\n",
    "    try:\n",
    "        shutil.rmtree(run_dir + \"/run_\" + str(isten))\n",
    "    except:\n",
    "        print(\"File: \" + run_dir + \"/run_\" + str(isten) + \", already deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e869a6-ce8e-4b53-9346-d25bfc7446f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite ave file up to nex\n",
    "\"\"\"\"\n",
    "nex = 10\n",
    "print(np.shape(X_all[:,:nex]))\n",
    "print(np.shape(Y_all[:,:nex]))\n",
    "print(np.shape(avg_powers_all[:nex]))\n",
    "\n",
    "X_all = X_all[:,:nex]\n",
    "Y_all = Y_all[:,:nex]\n",
    "avg_powers_all = avg_powers_all[:nex]\n",
    "\n",
    "filename_trainingdata = run_dir + '/' + filename_flipped_trainingdata\n",
    "nrw.save_training_data(X_all, Y_all, avg_powers_all, filename_trainingdata)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79da5299-4665-4ceb-8b66-4b793d3bb257",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 3, Use surrogate NN to pick low RMS from random inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031dec62-d5d3-45c8-9e12-994cacd16898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6aee649-2a79-42ac-bce4-3c08a13163e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 4, Use inverse NN to indentify low rms by inputing other low rms cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e4f0e-8662-4785-b271-26bf3d8c0b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd77d77f-e069-4822-92d9-72e0dd89cf39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 5, Genetic algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b6d4c-1890-4664-9dc9-1e6a1d92ceb1",
   "metadata": {},
   "source": [
    "Iterative procedure taking best features of first generation. Mutate and mix inputs between the best and produce subsequent generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1031cdc-d702-470d-9927-12d9ddf25713",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_points = 10\n",
    "n_iter = 10\n",
    "num_parallel = 10\n",
    "run_dir = \"Data_empty\"\n",
    "iter_dir = \"iter_\"\n",
    "filename_flipped_trainingdata = \"flipped_training_data_and_labels.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edd6056-6c92-447a-b61f-107df01c9e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the population size.\n",
    "pop_size = (num_inputs, init_points) # The population will have sol_per_pop chromosome where each chromosome has num_weights genes.\n",
    "#Creating the initial population.\n",
    "X_pop = rng.random(pop_size)\n",
    "num_parents_mating = int(init_points / 10.0)\n",
    "if (num_parents_mating % 2) != 0:\n",
    "    num_parents_mating -=1\n",
    "\n",
    "pbounds = np.zeros((num_inputs, 2))\n",
    "pbounds[:,1] = 1.0\n",
    "\n",
    "best_outputs = []\n",
    "X_all = np.array([], dtype=np.int64).reshape(num_inputs,0)\n",
    "Y_all= np.array([], dtype=np.int64).reshape(LMAX,0)\n",
    "avg_powers_all = np.array([], dtype=np.int64)\n",
    "tic = time.perf_counter()\n",
    "for generation in range(n_iter-1):\n",
    "    print(\"Generation : \", generation)\n",
    "    # Measuring the fitness of each chromosome in the population.\n",
    "    Y_pop, avg_powers_pop = tdg.run_ifriit_input(init_points, X_pop, run_dir, LMAX, num_parallel, hemisphere_symmetric, run_clean)\n",
    "    X_all = np.hstack((X_all, X_pop))\n",
    "    Y_all = np.hstack((Y_all, Y_pop))\n",
    "    avg_powers_all = np.hstack((avg_powers_all, avg_powers_pop))\n",
    "    for irun in range(init_points):\n",
    "        os.rename(run_dir + \"/run_\" + str(irun), run_dir + \"/\" + iter_dir + str(irun+init_points*generation))\n",
    "        \n",
    "    fitness_pop = -np.sqrt(np.sum(Y_pop**2, axis=0))\n",
    "    mindex_pop = np.argmax(fitness_pop)\n",
    "    \n",
    "    if (generation+1)%10 <= 0.0:\n",
    "        toc = time.perf_counter()\n",
    "        print(\"{:0.4f} seconds\".format(toc - tic))\n",
    "        print(str(generation+1) + \" genetic algorithm data points added, saving to .nc\")\n",
    "        filename_trainingdata = run_dir + '/' + filename_flipped_trainingdata\n",
    "        nrw.save_training_data(X_all, Y_all, avg_powers_all, filename_trainingdata)\n",
    "        mindex = np.argmin(np.mean(Y_all, axis=0))\n",
    "        print(mindex)\n",
    "        print(np.sum(Y_all[:,mindex]))\n",
    "        print(np.sqrt(np.sum(Y_all[:,mindex]**2)))\n",
    "        mindex = np.argmin(np.sqrt(np.sum(Y_all**2, axis=0)))\n",
    "        print(mindex)\n",
    "        print(np.sum(Y_all[:,mindex]))\n",
    "        print(np.sqrt(np.sum(Y_all[:,mindex]**2)))\n",
    "\n",
    "    best_outputs.append(np.max(fitness_pop))\n",
    "    # The best result in the current iteration.\n",
    "    print(\"Best result : \", best_outputs[generation])\n",
    "\n",
    "    # Selecting the best parents in the population for mating.\n",
    "    parents = uopt.select_mating_pool(X_pop.T, fitness_pop, num_parents_mating)\n",
    "\n",
    "    # Generating next generation using crossover.\n",
    "    offspring_crossover = uopt.crossover(parents, offspring_size=(init_points-num_parents_mating, num_inputs))\n",
    "\n",
    "    # Adding some variations to the offspring using mutation.\n",
    "    offspring_mutation = uopt.mutation(offspring_crossover, rng, pbounds, num_mutations=2)\n",
    "\n",
    "    # Creating the new population based on the parents and offspring.\n",
    "    X_pop[:,0:num_parents_mating] = parents.T\n",
    "    X_pop[:,num_parents_mating:] = offspring_mutation.T\n",
    "# Getting the best solution after iterating finishing all generations.\n",
    "#At first, the fitness is calculated for each solution in the final generation.\n",
    "Y_pop, avg_powers_pop = tdg.run_ifriit_input(init_points, X_pop, run_dir, LMAX, num_parallel, hemisphere_symmetric, run_clean)\n",
    "X_all = np.hstack((X_all, X_pop))\n",
    "Y_all = np.hstack((Y_all, Y_pop))\n",
    "avg_powers_all = np.hstack((avg_powers_all, avg_powers_pop))\n",
    "for irun in range(init_points):\n",
    "    os.rename(run_dir + \"/run_\" + str(irun), run_dir + \"/\" + iter_dir + str(irun+init_points*(generation+1)))\n",
    "\n",
    "# Then return the index of that solution corresponding to the best fitness.\n",
    "fitness_pop = -np.sqrt(np.sum(Y_pop**2, axis=0))\n",
    "mindex_pop = np.argmax(fitness_pop)\n",
    "\n",
    "print(\"Best solution : \", X_pop[:, mindex_pop])\n",
    "print(\"Best solution fitness : \", fitness_pop[mindex_pop])\n",
    "\n",
    "toc = time.perf_counter()\n",
    "print(\"{:0.4f} seconds\".format(toc - tic))\n",
    "print(str(generation+1) + \" genetic algorithm data points added, saving to .nc\")\n",
    "filename_trainingdata = run_dir + '/' + filename_flipped_trainingdata\n",
    "nrw.save_training_data(X_all, Y_all, avg_powers_all, filename_trainingdata)\n",
    "mindex = np.argmin(np.mean(Y_all, axis=0))\n",
    "print(mindex)\n",
    "print(np.sum(Y_all[:,mindex]))\n",
    "print(np.sqrt(np.sum(Y_all[:,mindex]**2)))\n",
    "mindex = np.argmin(np.sqrt(np.sum(Y_all**2, axis=0)))\n",
    "print(mindex)\n",
    "print(np.sum(Y_all[:,mindex]))\n",
    "print(np.sqrt(np.sum(Y_all[:,mindex]**2)))\n",
    "\n",
    "#plt.plot(best_outputs)\n",
    "#plt.xlabel(\"Iteration\")\n",
    "#plt.ylabel(\"Fitness\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba0f5b-722d-41ec-889e-3f41d91091c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 6, Bayesian optimization (ifriit is high quality source and NN is low quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9609d4-311e-4ee7-a61e-7b4e38c3bd77",
   "metadata": {},
   "source": [
    "Gaussian process surrogate and bayesian optimization used with multiple sources of information. First we create a bayesian model with the true data points and select new simulations based on that. The model (Kriging method?) could use a \"gaussian process approximation\" to reduce computational expense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce61426-7f1a-48f8-9d72-827e7b5c7262",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_points = num_examples\n",
    "n_iter = 10\n",
    "run_dir = \"Data_output\"\n",
    "iter_dir = \"iter_\"\n",
    "filename_flipped_trainingdata = \"flipped_training_data_and_labels.nc\"\n",
    "num_parallel = 10 # Not currently run parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b9ec6-f1b1-4bc4-843c-3e1460cd9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {}\n",
    "for ii in range(num_inputs):\n",
    "    pbounds[\"x\"+str(ii)] = (0., 1.)\n",
    "\n",
    "target = -np.sqrt(np.sum(Y_all**2, axis=0)) # Critical to make negative (min not max)\n",
    "print(num_inputs, init_points, np.shape(target))\n",
    "\n",
    "optimizer, utility = uopt.initialize_unknown_func(X_all, target, pbounds, init_points, num_inputs)\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d7a101-3d87-414f-bcef-f8484b05ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "\n",
    "tic = time.perf_counter()\n",
    "for ieval in range(start, n_iter):\n",
    "    next_point = optimizer.suggest(utility)\n",
    "\n",
    "    X_new = np.zeros((num_inputs, 1))\n",
    "    for ii in range(num_inputs):\n",
    "        X_new[ii] = next_point[\"x\"+str(ii)]\n",
    "    \n",
    "    Y_new, avg_powers_new = tdg.run_ifriit_input(1, X_new, run_dir, LMAX, num_parallel, hemisphere_symmetric, run_clean)\n",
    "    \n",
    "    X_all = np.hstack((X_all, X_new))\n",
    "    Y_all = np.hstack((Y_all, Y_new))\n",
    "    avg_powers_all = np.hstack((avg_powers_all, avg_powers_new))\n",
    "    \n",
    "    os.rename(run_dir + \"/run_0\", run_dir + \"/\" + iter_dir + str(ieval+num_examples))\n",
    "    \n",
    "    #target = black_box_function(**next_point)\n",
    "    target = -np.sqrt(np.sum(Y_new**2))\n",
    "    try:\n",
    "        optimizer.register(params=next_point, target=target)\n",
    "    except:\n",
    "        print(\"Broken input!\", next_point, target)\n",
    "    if (ieval+1)%10 <= 0.0:\n",
    "        toc = time.perf_counter()\n",
    "        print(\"{:0.4f} seconds\".format(toc - tic))\n",
    "        print(str(ieval+1) + \" Bayesian data points added, saving to .nc\")\n",
    "        filename_trainingdata = run_dir + '/' + filename_flipped_trainingdata\n",
    "        nrw.save_training_data(X_all, Y_all, avg_powers_all, filename_trainingdata)\n",
    "        print(optimizer.max)\n",
    "        mindex = np.argmin(np.mean(Y_all, axis=0))\n",
    "        print(mindex)\n",
    "        print(np.sum(Y_all[:,mindex]))\n",
    "        print(np.sqrt(np.sum(Y_all[:,mindex]**2)))\n",
    "        mindex = np.argmin(np.sqrt(np.sum(Y_all**2, axis=0)))\n",
    "        print(mindex)\n",
    "        print(np.sum(Y_all[:,mindex]))\n",
    "        print(np.sqrt(np.sum(Y_all[:,mindex]**2)))\n",
    "print(next_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3dba49-4564-4a4e-8dda-cb8fa35e88db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 7, Grid search algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0393c6d1-d0e5-4743-a7e6-a1bc95c26184",
   "metadata": {},
   "source": [
    "Split the entire search space into a grid (start coarse 2 or 3 cells per dimension) 3^16 = 43M. Evaluate each cell depending on the data points within or 8 nearest neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4101477-d30f-460f-9e81-f25e7d235136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f025763f-d7b4-498b-a1c4-ba2848fff404",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 8, Network search algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b292c18-5aa8-41c0-b3bb-f7a4ed751694",
   "metadata": {},
   "source": [
    "Find a gradient between all data points, use this information to initialize gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37784580-9e53-4183-ad8e-41bf06a96e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df5e1428-8dcf-479d-a30b-2fca0f4db9f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 9, Principle Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3eb066-8f05-4af0-ad97-28b1fed1a6d0",
   "metadata": {},
   "source": [
    "Combine with gradient descent for faster convergence? Enables plotting of dataset in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25c333-f2c6-40bf-8a8f-83d67f1722a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef247b59-ff08-4800-8a8c-4ef9d557e474",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 10, Transfer Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e486b0-da4f-4b91-b8dd-1c055f968a88",
   "metadata": {},
   "source": [
    "Generate low quality large dataset (1-50M examples?) using surrogate NN and use this for transfer learning. This might help to evaluate at what stage transfer learning becomes effective (can we use it with a dataset of 1000 or 10000?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00d0b3-c62c-4544-9a26-489690fbe104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
