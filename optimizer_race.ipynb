{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4325ff-71ae-46f4-aacc-a3107fb0c083",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Race to low rms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2945fbc-d119-407f-88e5-47731e7318fc",
   "metadata": {},
   "source": [
    "Import latin-hypercube test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0d5cdc-cd9e-4e66-b4e2-c70a66e3af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import netcdf_read_write as nrw\n",
    "import training_data_generation as tdg\n",
    "import utils_optimizers as uopt\n",
    "import optimize as opt\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "run_dir = \"Data_input\"\n",
    "trainingdata_filename = \"flipped_training_data_and_labels.nc\"\n",
    "num_modes = 30\n",
    "num_inputs = 16\n",
    "random_seed = 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9d5b9-619b-4c93-ac41-fbf22450ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_params = tdg.define_system_params(run_dir)\n",
    "sys_params[\"trainingdata_filename\"] = trainingdata_filename\n",
    "X_all, Y_all, avg_powers_all = nrw.import_training_data(sys_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4473ad-556f-45e6-abe0-9fdf24d4aaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_all = np.sqrt(np.sum(Y_all**2, axis=0))\n",
    "\n",
    "target_mean = np.mean(target_all)\n",
    "target_variance = np.sqrt(np.var(target_all))\n",
    "print(target_mean*100.0, target_variance*100.0)\n",
    "\n",
    "input_means = np.mean(X_all, axis=1)\n",
    "input_standard_deviation = np.sqrt(np.var(X_all, axis=1))\n",
    "print(input_means*100.0, input_standard_deviation*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6857d4-5f18-4461-84d6-fb9a09e5dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(X_all[:,0]), np.shape(Y_all[:,0]))\n",
    "mindex = np.argmin(np.mean(Y_all, axis=0))\n",
    "print(mindex)\n",
    "print(np.sum(Y_all[:,mindex]))\n",
    "print(target_all[mindex])\n",
    "mindex = np.argmin(target_all)\n",
    "print(mindex)\n",
    "print(np.sum(Y_all[:,mindex]))\n",
    "print(target_all[mindex])\n",
    "\n",
    "num_init_examples = np.shape(X_all)[1]\n",
    "print(num_init_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c95c5-32a6-4cf8-aeb5-a1956c662522",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "plt.plot(np.arange(num_modes), Y_all[:,mindex] * 100.0)\n",
    "ax.set_xticks(range(0, num_modes+1, int(num_modes/5)))\n",
    "plt.xlim([0, num_modes])\n",
    "plt.title(\"Unweighted Modes\")\n",
    "plt.xlabel(\"l mode\")\n",
    "plt.ylabel(r\"amplitude ($\\%$)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d030fdb1-4c00-47a3-9b81-fdcea09adfba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Evaluate Minimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f251430-80ae-444b-8880-d855c9a0714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs_for_comparison = [\"Data_221122a_ga_1kex_bo_gd\", \"Data_221125d_all_gradientdescent_1000ex\",\"Data_221116c_ga_10kex_100x100_fewer_parents\",\"Data_221201a_all_bayesian_5000ex_gd_1000ex\"]#\"Data_221125c_all_bayesian_5000ex\",\n",
    "colours = [\"b\",\"g\",\"r\",\"m\",\"c\"]\n",
    "optimization_label = [\"Mixed\",\"All gradient descent\",\"All genetic algorithm\",\"Bayesian with gradient descent\"]#,\"All bayesian\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef96b3f-9258-49c3-91f4-4fc0e195a81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "ax = plt.axes()\n",
    "\n",
    "plt.xlabel(\"Number of simulations\")\n",
    "plt.ylabel(r\"RMS Statistics for moving window size: \" + str(window_size) + r\" ($\\%$)\");\n",
    "data_set = 0\n",
    "for run_dir in dirs_for_comparison:\n",
    "    sys_params = tdg.define_system_params(run_dir)\n",
    "    sys_params[\"trainingdata_filename\"] = trainingdata_filename\n",
    "    X_all, Y_all, avg_powers_all = nrw.import_training_data(sys_params)\n",
    "    target_all = np.sqrt(np.sum(Y_all**2, axis=0))\n",
    "\n",
    "    target_mean = np.mean(target_all)\n",
    "    target_variance = np.sqrt(np.var(target_all))\n",
    "\n",
    "    input_means = np.mean(X_all, axis=1)\n",
    "    input_standard_deviation = np.sqrt(np.var(X_all, axis=1))\n",
    "\n",
    "    window_size = 100\n",
    "  \n",
    "    i = 0\n",
    "    # Initialize an empty list to store moving averages\n",
    "    moving_averages = []\n",
    "    moving_min = []\n",
    "  \n",
    "    # Loop through the array t o\n",
    "    #consider every window of size 3\n",
    "    while i < len(target_all) - window_size + 1:\n",
    "        # taken from https://www.geeksforgeeks.org/how-to-calculate-moving-averages-in-python/\n",
    "  \n",
    "        window_average = np.sum(target_all[i:i+window_size]) / window_size\n",
    "        moving_averages.append(window_average)\n",
    "    \n",
    "        window_min = np.min(target_all[i:i+window_size])\n",
    "        moving_min.append(window_min)\n",
    "      \n",
    "        # Shift window to right by one position\n",
    "        i += 1\n",
    "    moving_averages = np.array(moving_averages)\n",
    "    moving_min = np.array(moving_min)\n",
    "    \n",
    "    plt.semilogy(moving_averages * 100.0, linestyle=\"dotted\", color=colours[data_set], label=optimization_label[data_set]+\" (mean)\")\n",
    "    plt.semilogy(moving_min * 100.0, linestyle=\"solid\", color=colours[data_set], label=optimization_label[data_set]+\" (min)\")\n",
    "    data_set += 1\n",
    "plt.legend();\n",
    "plt.savefig(\"Compare_optimization_techniques\" + sys_params[\"plot_file_type\"], dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff8ef3-69fb-42b2-b223-f827dd5d65b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 1, Brute force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1ab928-25b7-40d7-9491-4fc4e821de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e8c427-27ad-4e0e-bd90-70d47385ab61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b01aa36-7861-4579-8661-aef3dcba1048",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 2, Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c2f2f7-6611-4457-b6b0-ce4ca6f64853",
   "metadata": {},
   "source": [
    "The partial derivative is determined using a 2*16=32 grid of points (2 points in every dimension) around the current minima. These points can be evaluated in either a NN or Ifriit depending on speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de2225-af14-4b2a-9d77-2f51f29b775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"Data_output\"\n",
    "num_parallel = 11\n",
    "n_iter = 10\n",
    "optimizer_params = uopt.define_optimizer_parameters(root_dir, num_inputs, num_modes,\n",
    "                                                    num_init_examples, n_iter, num_parallel, random_seed)\n",
    "dataset = uopt.define_optimizer_dataset(X_all, Y_all, avg_powers_all)\n",
    "gd_params = uopt.define_gradient_descent_params(num_parallel)\n",
    "dataset = opt.wrapper_gradient_descent(dataset, gd_params, optimizer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79da5299-4665-4ceb-8b66-4b793d3bb257",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 3, Use surrogate NN to pick low RMS from random inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031dec62-d5d3-45c8-9e12-994cacd16898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6aee649-2a79-42ac-bce4-3c08a13163e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 4, Use inverse NN to indentify low rms by inputing other low rms cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e4f0e-8662-4785-b271-26bf3d8c0b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd77d77f-e069-4822-92d9-72e0dd89cf39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 5, Genetic algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b6d4c-1890-4664-9dc9-1e6a1d92ceb1",
   "metadata": {},
   "source": [
    "Iterative procedure taking best features of first generation. Mutate and mix inputs between the best and produce subsequent generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3013626-4845-4c9e-a2bb-51124289e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"Data_output\"\n",
    "n_iter = 10\n",
    "init_points = 10\n",
    "num_parallel = 10\n",
    "X_all = np.array([], dtype=np.int64).reshape(num_inputs,0)\n",
    "Y_all= np.array([], dtype=np.int64).reshape(num_modes,0)\n",
    "avg_powers_all = np.array([], dtype=np.int64)\n",
    "\n",
    "num_parents_mating = int(init_points / 10.0)\n",
    "if (num_parents_mating % 2) != 0:\n",
    "    num_parents_mating -=1\n",
    "if num_parents_mating < 2:\n",
    "    num_parents_mating = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582f4627-d04c-4868-999e-dea52f7c9244",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_init_examples = 0 # genetic algorithm generates it's own intial data\n",
    "opt_params = uopt.define_optimizer_parameters(root_dir, num_inputs, num_modes,\n",
    "                                              num_init_examples, n_iter, num_parallel, random_seed)\n",
    "dataset = uopt.define_optimizer_dataset(X_all, Y_all, avg_powers_all)\n",
    "ga_params = uopt.define_genetic_algorithm_params(init_points, num_parents_mating)\n",
    "dataset = opt.wrapper_genetic_algorithm(dataset, ga_params, opt_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba0f5b-722d-41ec-889e-3f41d91091c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 6, Bayesian optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9609d4-311e-4ee7-a61e-7b4e38c3bd77",
   "metadata": {},
   "source": [
    "Gaussian process surrogate and bayesian optimization used with multiple sources of information. First we create a bayesian model with the true data points and select new simulations based on that. The model (Kriging method?) could use a \"gaussian process approximation\" to reduce computational expense.\n",
    "\n",
    "Future extension: multi-source bayesian optimization, (ifriit is high quality source and NN is low quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41a085e-4147-44f8-ab0f-62f3d982ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"Data_output\"\n",
    "n_iter = 10\n",
    "num_parallel = 10\n",
    "target_set_undetermined = - target_mean / 2.0\n",
    "opt_params = uopt.define_optimizer_parameters(root_dir, num_inputs, num_modes,\n",
    "                                              num_init_examples, n_iter, num_parallel, random_seed)\n",
    "dataset = uopt.define_optimizer_dataset(X_all, Y_all, avg_powers_all)\n",
    "bo_params = uopt.define_bayesian_optimisation_params(target_set_undetermined)\n",
    "dataset = opt.wrapper_bayesian_optimisation(dataset, bo_params, opt_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3dba49-4564-4a4e-8dda-cb8fa35e88db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 7, Grid search algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0393c6d1-d0e5-4743-a7e6-a1bc95c26184",
   "metadata": {},
   "source": [
    "Split the entire search space into a grid (start coarse 2 or 3 cells per dimension) 3^16 = 43M. Evaluate each cell depending on the data points within or 8 nearest neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4101477-d30f-460f-9e81-f25e7d235136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f025763f-d7b4-498b-a1c4-ba2848fff404",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 8, Network search algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b292c18-5aa8-41c0-b3bb-f7a4ed751694",
   "metadata": {},
   "source": [
    "Find a gradient between all data points, use this information to initialize gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37784580-9e53-4183-ad8e-41bf06a96e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df5e1428-8dcf-479d-a30b-2fca0f4db9f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 9, Principle Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3eb066-8f05-4af0-ad97-28b1fed1a6d0",
   "metadata": {},
   "source": [
    "Combine with gradient descent for faster convergence? Enables plotting of dataset in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25c333-f2c6-40bf-8a8f-83d67f1722a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef247b59-ff08-4800-8a8c-4ef9d557e474",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 10, Transfer Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e486b0-da4f-4b91-b8dd-1c055f968a88",
   "metadata": {},
   "source": [
    "Generate low quality large dataset (1-50M examples?) using surrogate NN and use this for transfer learning. This might help to evaluate at what stage transfer learning becomes effective (can we use it with a dataset of 1000 or 10000?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00d0b3-c62c-4544-9a26-489690fbe104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
