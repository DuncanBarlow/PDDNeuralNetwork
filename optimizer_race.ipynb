{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4325ff-71ae-46f4-aacc-a3107fb0c083",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Race to low rms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2945fbc-d119-407f-88e5-47731e7318fc",
   "metadata": {},
   "source": [
    "Import latin-hypercube test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0d5cdc-cd9e-4e66-b4e2-c70a66e3af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import healpy as hp\n",
    "import utils_intensity_map as uim\n",
    "import utils_deck_generation as idg\n",
    "import netcdf_read_write as nrw\n",
    "import training_data_generation as tdg\n",
    "import tf_neural_network as tfnn\n",
    "import neural_network_generation as nng\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "run_dir = \"Data_input\"\n",
    "num_nn = 1\n",
    "\n",
    "filename_flipped_trainingdata = \"flipped_training_data_and_labels.nc\"\n",
    "imap_nside = 256\n",
    "LMAX = 30\n",
    "hemisphere_symmetric = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9d5b9-619b-4c93-ac41-fbf22450ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_params = tdg.define_system_params(run_dir)\n",
    "nn_params = nng.define_nn_params(num_nn)\n",
    "sys_params[\"trainingdata_filename\"] = filename_flipped_trainingdata\n",
    "X_all, Y_all, avg_powers_all, nn_params = nng.import_training_data(nn_params, sys_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6857d4-5f18-4461-84d6-fb9a09e5dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(X_all[:,0]), np.shape(Y_all[:,0]))\n",
    "mindex = np.argmin(np.mean(Y_all, axis=0))\n",
    "print(mindex)\n",
    "print(np.sum(Y_all[:,mindex]))\n",
    "print(np.sqrt(np.sum(Y_all[:,mindex]**2)))\n",
    "mindex = np.argmin(np.sqrt(np.sum(Y_all**2, axis=0)))\n",
    "print(mindex)\n",
    "print(np.sum(Y_all[:,mindex]))\n",
    "print(np.sqrt(np.sum(Y_all[:,mindex]**2)))\n",
    "\n",
    "num_examples = np.shape(X_all)[1]\n",
    "print(num_examples)\n",
    "num_inputs = np.shape(X_all)[0]\n",
    "num_modes = np.shape(Y_all)[0]\n",
    "print(num_inputs, num_modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c95c5-32a6-4cf8-aeb5-a1956c662522",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "plt.plot(np.arange(LMAX), Y_all[:,mindex] * 100.0)\n",
    "ax.set_xticks(range(0, LMAX+1, int(LMAX/5)))\n",
    "plt.xlim([0, LMAX])\n",
    "plt.title(\"Unweighted Modes\")\n",
    "plt.xlabel(\"l mode\")\n",
    "plt.ylabel(r\"amplitude ($\\%$)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441f6a3e-d6e9-4b57-b1a9-544e46929c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ifriit_input(num_examples, X_all, run_dir, LMAX, num_nn, num_parallel, hemisphere_symmetric):\n",
    "    dataset_params, facility_spec = tdg.define_dataset_params(num_examples)\n",
    "    dataset_params[\"hemisphere_symmetric\"] = hemisphere_symmetric\n",
    "    dataset_params[\"Y_train\"] = X_all\n",
    "\n",
    "    sys_params = tdg.define_system_params(run_dir)\n",
    "    sys_params[\"num_processes\"] = num_parallel\n",
    "    sys_params[\"run_clean\"] = True # Create new run files\n",
    "    \n",
    "    dataset_params = idg.create_run_files(dataset_params, sys_params, facility_spec)\n",
    "    tdg.generate_training_data(dataset_params, sys_params, facility_spec)\n",
    "    \n",
    "    nn_params = nng.define_nn_params(num_nn)\n",
    "    X_all, Y_all, avg_powers_all, nn_params = nng.import_training_data_reversed(nn_params, sys_params, LMAX)\n",
    "    return Y_all, avg_powers_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff8ef3-69fb-42b2-b223-f827dd5d65b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 1, Brute force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1ab928-25b7-40d7-9491-4fc4e821de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e8c427-27ad-4e0e-bd90-70d47385ab61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b01aa36-7861-4579-8661-aef3dcba1048",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 2, Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c2f2f7-6611-4457-b6b0-ce4ca6f64853",
   "metadata": {},
   "source": [
    "The partial derivative is determined using a 2*16=32 grid of points (2 points in every dimension) around the current minima. These points can be evaluated in either a NN or Ifriit depending on speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd242ea-0a38-4327-b331-6094651fb2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_stencil(X_new, learning_rate, pbounds, num_inputs, stencil_size):\n",
    "    X_stencil = np.zeros((num_inputs, stencil_size))\n",
    "    \n",
    "    counter = 0\n",
    "    X_stencil[:, counter] = X_new[:,0]\n",
    "    counter += 1\n",
    "    for ii in range(num_inputs):\n",
    "        X_stencil[:, counter] = X_new[:,0]\n",
    "        X_stencil[ii, counter] = X_new[ii,0] - learning_rate\n",
    "        if (X_stencil[ii,counter] < pbounds[ii,0]):\n",
    "            X_stencil[ii,counter] = pbounds[ii,0] # to avoid stencil leaving domain\n",
    "        counter += 1\n",
    "        X_stencil[:, counter] = X_new[:,0]\n",
    "        X_stencil[ii, counter] = X_new[ii,0] + learning_rate\n",
    "        if (X_stencil[ii,counter] > pbounds[ii,1]):\n",
    "            X_stencil[ii,counter] = pbounds[ii,1] # to avoid stencil leaving domain\n",
    "        counter += 1\n",
    "\n",
    "    return X_stencil\n",
    "\n",
    "\n",
    "\n",
    "def determine_gradient(X_stencil, target, learning_rate, pbounds, num_inputs):\n",
    "\n",
    "    grad = np.zeros(num_inputs)\n",
    "    counter = 0\n",
    "    f_centre = target[counter]\n",
    "    counter += 1\n",
    "    for ii in range(num_inputs):\n",
    "\n",
    "        centred_diff = True\n",
    "        forward_diff = False\n",
    "        backward_diff = False\n",
    "        \n",
    "        if (X_stencil[ii,counter] < pbounds[ii,0]):\n",
    "            centred_diff = False\n",
    "            forward_diff = True \n",
    "        else:\n",
    "            f_minus = target[counter]\n",
    "        counter += 1\n",
    "        \n",
    "        if (X_stencil[ii,counter] > pbounds[ii,1]):\n",
    "            centred_diff = False\n",
    "            backward_diff = True\n",
    "        else:\n",
    "            f_plus = target[counter]\n",
    "        counter += 1\n",
    "        \n",
    "        if centred_diff:\n",
    "            grad[ii] = (f_plus - f_minus) / (2.0 * learning_rate)\n",
    "        elif forward_diff:\n",
    "            grad[ii] = (f_plus - f_centre) / learning_rate\n",
    "        elif backward_diff:\n",
    "            grad[ii] = (f_centre - f_minus) / learning_rate\n",
    "        else:\n",
    "            grad[ii] = 0.0\n",
    "            print(\"Broken gradients!\")\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "\n",
    "def grad_descent(X_old, grad, step_size, pbounds, num_inputs, num_steps_per_iter):\n",
    "    \n",
    "    learning_rates = np.logspace(step_size[0], step_size[1], num_steps_per_iter)\n",
    "    X_new = np.zeros((num_inputs, num_steps_per_iter))\n",
    "    for ieval in range(num_steps_per_iter):\n",
    "        X_new[:,ieval] = X_old[:,0]\n",
    "        for ii in range(num_inputs):\n",
    "            X_new[ii,ieval] = X_old[ii,0] - learning_rates[ieval] * grad[ii]\n",
    "            if (X_new[ii,ieval] < pbounds[ii,0]):\n",
    "                X_new[ii,ieval] = pbounds[ii,0]\n",
    "            elif (X_new[ii,ieval] > pbounds[ii,1]):\n",
    "                X_new[ii,ieval] = pbounds[ii,1]\n",
    "\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de2225-af14-4b2a-9d77-2f51f29b775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_exp = -1.0\n",
    "n_iter = 10\n",
    "run_dir = \"Data_output\"\n",
    "iter_dir = \"iter_\"\n",
    "filename_flipped_trainingdata = \"flipped_training_data_and_labels.nc\"\n",
    "num_parallel = 17\n",
    "stencil_size = num_inputs * 2 + 1\n",
    "num_steps_per_iter = num_parallel - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a90c871-3f77-44cb-a496-e5be04d7a1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 10.0**learn_exp\n",
    "step_size = np.array([learn_exp - 1.0, learn_exp + 1.0])\n",
    "\n",
    "X_old = np.zeros((num_inputs, 1))\n",
    "Y_old = np.zeros((num_modes, 1))\n",
    "avg_powers_old = np.array([0.0])\n",
    "\n",
    "mindex = np.argmin(np.sqrt(np.sum(Y_all**2, axis=0)))\n",
    "X_old[:,0] = X_all[:, mindex]\n",
    "\n",
    "pbounds = np.zeros((num_inputs, 2))\n",
    "pbounds[:,1] = 1.0\n",
    "tic = time.perf_counter()\n",
    "for ieval in range(n_iter):\n",
    "    \n",
    "    if (sum(abs(X_all[:,-1] - X_all[:,-2])) <= 0.0):\n",
    "        learn_exp = learn_exp-0.5\n",
    "        learning_rate = 10.0**(learn_exp)\n",
    "        step_size = step_size - 0.5\n",
    "        print(\"Reducing step size to: \" + str(learning_rate))\n",
    "        if learning_rate < 1.0e-4:\n",
    "            print(str(ieval+1) + \" Bayesian data points added, saving to .nc\")\n",
    "            print(\"Early stopping due to repeated results\")\n",
    "            filename_trainingdata = run_dir + '/' + filename_flipped_trainingdata\n",
    "            nrw.save_training_data(X_all, Y_all, avg_powers_all, filename_trainingdata)\n",
    "            break\n",
    "    \n",
    "    X_stencil = gradient_stencil(X_old, learning_rate, pbounds, num_inputs, stencil_size)\n",
    "    Y_stencil, avg_powers_stencil = run_ifriit_input(stencil_size, X_stencil, run_dir, LMAX, num_nn, num_parallel, hemisphere_symmetric)\n",
    "    target_stencil = np.sqrt(np.sum(Y_stencil**2, axis=0))\n",
    "    mindex_stencil = np.argmin(target_stencil)\n",
    "    print(\"The minimum in the stencil\", np.min(target_stencil), mindex_stencil)\n",
    "    print(\"The previous value was: \", target_stencil[0], 0)\n",
    "    print(X_stencil[:,0])\n",
    "    os.rename(run_dir + \"/run_\" + str(mindex_stencil), run_dir + \"/\" + iter_dir + str(ieval))\n",
    "    \n",
    "    grad = determine_gradient(X_stencil, target_stencil, learning_rate, pbounds, num_inputs)\n",
    "    X_new = grad_descent(X_old, grad, step_size, pbounds, num_inputs, num_steps_per_iter)\n",
    "    \n",
    "    Y_new, avg_powers_new = run_ifriit_input(num_steps_per_iter, X_new, run_dir, LMAX, num_nn, num_parallel, hemisphere_symmetric)\n",
    "    target_downhill = np.sqrt(np.sum(Y_new**2, axis=0))\n",
    "    mindex_downhill = np.argmin(target_downhill)\n",
    "    print(\"The minimum downhill\", np.min(target_downhill), mindex_downhill)\n",
    "    \n",
    "    if target_downhill[mindex_downhill] < target_stencil[mindex_stencil]:\n",
    "        shutil.rmtree(run_dir + \"/\" + iter_dir + str(ieval))\n",
    "        os.rename(run_dir + \"/run_\" + str(mindex_downhill), run_dir + \"/\" + iter_dir + str(ieval))\n",
    "        X_old[:,0] = X_new[:,mindex_downhill]\n",
    "        Y_old[:,0] = Y_new[:,mindex_downhill]\n",
    "        avg_powers_old = avg_powers_new[mindex_downhill]\n",
    "    else:\n",
    "        X_old[:,0] = X_stencil[:,mindex_stencil]\n",
    "        Y_old[:,0] = Y_stencil[:,mindex_stencil]\n",
    "        avg_powers_old = avg_powers_stencil[mindex_stencil]\n",
    "    \n",
    "    X_all = np.hstack((X_all, X_old))\n",
    "    Y_all = np.hstack((Y_all, Y_old))\n",
    "    avg_powers_all = np.hstack((avg_powers_all, avg_powers_old))\n",
    "    \n",
    "    print(\"Iteration {} with learn rate {} value:{}\".format(ieval, learning_rate, np.sqrt(np.sum(Y_old**2))))\n",
    "    print(X_old[:,0])\n",
    "    \n",
    "    if (np.sqrt(np.sum(Y_all[:,-1]**2)) > np.sqrt(np.sum(Y_all[:,-2]**2))):\n",
    "        print(\"Bug! Ascending slope!\")\n",
    "        print(np.sqrt(np.sum(Y_all[:,-1]**2)), np.sqrt(np.sum(Y_all[:,-2]**2)))\n",
    "        break\n",
    "    \n",
    "    if (ieval+1)%10 <= 0.0:\n",
    "        toc = time.perf_counter()\n",
    "        print(\"{:0.4f} seconds\".format(toc - tic))\n",
    "        print(str(ieval+1) + \" Bayesian data points added, saving to .nc\")\n",
    "        filename_trainingdata = run_dir + '/' + filename_flipped_trainingdata\n",
    "        nrw.save_training_data(X_all, Y_all, avg_powers_all, filename_trainingdata)\n",
    "        mindex = np.argmin(np.mean(Y_all, axis=0))\n",
    "        print(mindex)\n",
    "        print(np.sum(Y_all[:,mindex]))\n",
    "        print(np.sqrt(np.sum(Y_all[:,mindex]**2)))\n",
    "        mindex = np.argmin(np.sqrt(np.sum(Y_all**2, axis=0)))\n",
    "        print(mindex)\n",
    "        print(np.sum(Y_all[:,mindex]))\n",
    "        print(np.sqrt(np.sum(Y_all[:,mindex]**2)))\n",
    "for isten in range(stencil_size):\n",
    "    try:\n",
    "        shutil.rmtree(run_dir + \"/run_\" + str(isten))\n",
    "    except:\n",
    "        print(\"File: \" + run_dir + \"/run_\" + str(isten) + \", already deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e869a6-ce8e-4b53-9346-d25bfc7446f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite ave file up to nex\n",
    "\"\"\"\"\n",
    "nex = 10\n",
    "print(np.shape(X_all[:,:nex]))\n",
    "print(np.shape(Y_all[:,:nex]))\n",
    "print(np.shape(avg_powers_all[:nex]))\n",
    "\n",
    "X_all = X_all[:,:nex]\n",
    "Y_all = Y_all[:,:nex]\n",
    "avg_powers_all = avg_powers_all[:nex]\n",
    "\n",
    "filename_trainingdata = run_dir + '/' + filename_flipped_trainingdata\n",
    "nrw.save_training_data(X_all, Y_all, avg_powers_all, filename_trainingdata)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79da5299-4665-4ceb-8b66-4b793d3bb257",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 3, Use surrogate NN to pick low RMS from random inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031dec62-d5d3-45c8-9e12-994cacd16898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6aee649-2a79-42ac-bce4-3c08a13163e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 4, Use inverse NN to indentify low rms by inputing other low rms cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e4f0e-8662-4785-b271-26bf3d8c0b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd77d77f-e069-4822-92d9-72e0dd89cf39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 5, Genetic algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b6d4c-1890-4664-9dc9-1e6a1d92ceb1",
   "metadata": {},
   "source": [
    "Iterative procedure taking best features of first generation. Mutate and mix inputs between the best and produce subsequent generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3165c31-f9e0-4ed4-8411-f5cf7cc1e8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0ba0f5b-722d-41ec-889e-3f41d91091c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 6, Bayesian optimization (ifriit is high quality source and NN is low quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9609d4-311e-4ee7-a61e-7b4e38c3bd77",
   "metadata": {},
   "source": [
    "Gaussian process surrogate and bayesian optimization used with multiple sources of information. First we create a bayesian model with the true data points and select new simulations based on that. The model (Kriging method?) could use a \"gaussian process approximation\" to reduce computational expense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce61426-7f1a-48f8-9d72-827e7b5c7262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrapper_bayesian_optimizer as wbo\n",
    "\n",
    "init_points = num_examples\n",
    "n_iter = 10\n",
    "run_dir = \"Data_output\"\n",
    "iter_dir = \"iter_\"\n",
    "filename_flipped_trainingdata = \"flipped_training_data_and_labels.nc\"\n",
    "num_parallel = 10 # Not currently run parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b9ec6-f1b1-4bc4-843c-3e1460cd9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {}\n",
    "for ii in range(num_inputs):\n",
    "    pbounds[\"x\"+str(ii)] = (0., 1.)\n",
    "\n",
    "target = -np.sqrt(np.sum(Y_all**2, axis=0)) # Critical to make negative (min not max)\n",
    "print(num_inputs, init_points, np.shape(target))\n",
    "\n",
    "optimizer, utility = wbo.initialize_unknown_func(X_all, target, pbounds, init_points, num_inputs)\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d7a101-3d87-414f-bcef-f8484b05ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "\n",
    "tic = time.perf_counter()\n",
    "for ieval in range(start, n_iter):\n",
    "    next_point = optimizer.suggest(utility)\n",
    "\n",
    "    X_new = np.zeros((num_inputs, 1))\n",
    "    for ii in range(num_inputs):\n",
    "        X_new[ii] = next_point[\"x\"+str(ii)]\n",
    "    \n",
    "    Y_new, avg_powers_new = run_ifriit_input(1, X_new, run_dir, LMAX, num_nn, num_parallel, hemisphere_symmetric)\n",
    "    \n",
    "    X_all = np.hstack((X_all, X_new))\n",
    "    Y_all = np.hstack((Y_all, Y_new))\n",
    "    avg_powers_all = np.hstack((avg_powers_all, avg_powers_new))\n",
    "    \n",
    "    os.rename(run_dir + \"/run_0\", run_dir + \"/\" + iter_dir + str(ieval))\n",
    "    \n",
    "    #target = black_box_function(**next_point)\n",
    "    target = -np.sqrt(np.sum(Y_new**2))\n",
    "    try:\n",
    "        optimizer.register(params=next_point, target=target)\n",
    "    except:\n",
    "        print(\"Broken input!\", next_point, target)\n",
    "    if (ieval+1)%10 <= 0.0:\n",
    "        toc = time.perf_counter()\n",
    "        print(\"{:0.4f} seconds\".format(toc - tic))\n",
    "        print(str(ieval+1) + \" Bayesian data points added, saving to .nc\")\n",
    "        filename_trainingdata = run_dir + '/' + filename_flipped_trainingdata\n",
    "        nrw.save_training_data(X_all, Y_all, avg_powers_all, filename_trainingdata)\n",
    "        print(optimizer.max)\n",
    "        mindex = np.argmin(np.mean(Y_all, axis=0))\n",
    "        print(mindex)\n",
    "        print(np.sum(Y_all[:,mindex]))\n",
    "        print(np.sqrt(np.sum(Y_all[:,mindex]**2)))\n",
    "        mindex = np.argmin(np.sqrt(np.sum(Y_all**2, axis=0)))\n",
    "        print(mindex)\n",
    "        print(np.sum(Y_all[:,mindex]))\n",
    "        print(np.sqrt(np.sum(Y_all[:,mindex]**2)))\n",
    "print(next_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3dba49-4564-4a4e-8dda-cb8fa35e88db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 7, Grid search algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0393c6d1-d0e5-4743-a7e6-a1bc95c26184",
   "metadata": {},
   "source": [
    "Split the entire search space into a grid (start coarse 2 or 3 cells per dimension) 3^16 = 43M. Evaluate each cell depending on the data points within or 8 nearest neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4101477-d30f-460f-9e81-f25e7d235136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f025763f-d7b4-498b-a1c4-ba2848fff404",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 8, Network search algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b292c18-5aa8-41c0-b3bb-f7a4ed751694",
   "metadata": {},
   "source": [
    "Find a gradient between all data points, use this information to initialize gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37784580-9e53-4183-ad8e-41bf06a96e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df5e1428-8dcf-479d-a30b-2fca0f4db9f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 9, Principle Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3eb066-8f05-4af0-ad97-28b1fed1a6d0",
   "metadata": {},
   "source": [
    "Combine with gradient descent for faster convergence? Enables plotting of dataset in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25c333-f2c6-40bf-8a8f-83d67f1722a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef247b59-ff08-4800-8a8c-4ef9d557e474",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 10, Transfer Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e486b0-da4f-4b91-b8dd-1c055f968a88",
   "metadata": {},
   "source": [
    "Generate low quality large dataset (1-50M examples?) using surrogate NN and use this for transfer learning. This might help to evaluate at what stage transfer learning becomes effective (can we use it with a dataset of 1000 or 10000?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00d0b3-c62c-4544-9a26-489690fbe104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
