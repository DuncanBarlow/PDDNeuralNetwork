{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4325ff-71ae-46f4-aacc-a3107fb0c083",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Race to low rms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2945fbc-d119-407f-88e5-47731e7318fc",
   "metadata": {},
   "source": [
    "Import latin-hypercube test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0d5cdc-cd9e-4e66-b4e2-c70a66e3af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import healpy as hp\n",
    "import utils_intensity_map as uim\n",
    "import utils_deck_generation as idg\n",
    "import netcdf_read_write as nrw\n",
    "import training_data_generation as tdg\n",
    "import tf_neural_network as tfnn\n",
    "import neural_network_generation as nng\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "run_dir = \"Data_221028_symmetric_hemispheres\"\n",
    "num_nn = 1\n",
    "\n",
    "filename_flipped_trainingdata = \"flipped_training_data_and_labels.nc\"\n",
    "imap_nside = 256\n",
    "LMAX = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9d5b9-619b-4c93-ac41-fbf22450ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_params = tdg.define_system_params(run_dir)\n",
    "nn_params = nng.define_nn_params(num_nn)\n",
    "sys_params[\"trainingdata_filename\"] = filename_flipped_trainingdata\n",
    "X_all, Y_all, avg_powers_all, nn_params = nng.import_training_data(nn_params, sys_params)\n",
    "#nn_dataset = nng.seperate_test_set(X_all, Y_all, avg_powers_all, nn_params)\n",
    "num_examples = np.shape(X_all)[1]\n",
    "print(num_examples)\n",
    "num_inputs = np.shape(X_all)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6857d4-5f18-4461-84d6-fb9a09e5dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_all[:,0], Y_all[:,0])\n",
    "mindex = np.argmin(np.mean(Y_all, axis=0))\n",
    "print(mindex)\n",
    "print(np.sum(Y_all[:,mindex]))\n",
    "print(np.sqrt(np.sum(Y_all[:,mindex]**2)))\n",
    "mindex = np.argmin(np.sqrt(np.sum(Y_all**2, axis=0)))\n",
    "print(mindex)\n",
    "print(np.sum(Y_all[:,mindex]))\n",
    "print(np.sqrt(np.sum(Y_all[:,mindex]**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c95c5-32a6-4cf8-aeb5-a1956c662522",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "plt.plot(np.arange(LMAX), Y_all[:,mindex] * 100.0)\n",
    "ax.set_xticks(range(0, LMAX+1, int(LMAX/5)))\n",
    "plt.xlim([0, LMAX])\n",
    "plt.title(\"Unweighted Modes\")\n",
    "plt.xlabel(\"l mode\")\n",
    "plt.ylabel(r\"amplitude ($\\%$)\");\n",
    "#plt.savefig(sys_params[\"figure_location\"]+\"/unweighted_modes_\"+str(mindex)+\"original\" + sys_params[\"plot_file_type\"], dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441f6a3e-d6e9-4b57-b1a9-544e46929c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ifriit_input(num_examples, X_all, run_dir, LMAX, num_nn):\n",
    "    dataset_params, facility_spec = tdg.define_dataset_params(num_examples)\n",
    "    dataset_params[\"Y_train\"] = X_all\n",
    "\n",
    "    sys_params = tdg.define_system_params(run_dir)\n",
    "\n",
    "    # Create new run files\n",
    "    sys_params[\"run_clean\"] = False\n",
    "    dataset_params = idg.create_run_files(dataset_params, sys_params, facility_spec)\n",
    "    tdg.generate_training_data(dataset_params, sys_params, facility_spec)\n",
    "    \n",
    "    nn_params = nng.define_nn_params(num_nn)\n",
    "    X_all, Y_all, avg_powers_all, nn_params = nng.import_training_data_reversed(nn_params, sys_params, LMAX)\n",
    "    return Y_all, avg_powers_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff8ef3-69fb-42b2-b223-f827dd5d65b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 1, Brute force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1ab928-25b7-40d7-9491-4fc4e821de9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e8c427-27ad-4e0e-bd90-70d47385ab61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b01aa36-7861-4579-8661-aef3dcba1048",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Method 2, Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c2f2f7-6611-4457-b6b0-ce4ca6f64853",
   "metadata": {},
   "source": [
    "The partial derivative is determined using a 2*16=32 grid of points (2 points in every dimension) around the current minima. These points can be evaluated in either a NN or Ifriit depending on speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd242ea-0a38-4327-b331-6094651fb2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_gradient(point_dict, learning_rate, pbounds, num_inputs):\n",
    "    learning_rate = learning_rate\n",
    "    counter = 0\n",
    "\n",
    "    point_neighbours = point_dict\n",
    "    grad = {}\n",
    "    for ii in range(num_inputs):\n",
    "        centred_diff = True\n",
    "        forward_diff = False\n",
    "        backward_diff = False\n",
    "\n",
    "        point_neighbours[\"x\"+str(ii)] = point_dict[\"x\"+str(ii)] - learning_rate\n",
    "        if (point_neighbours[\"x\"+str(ii)] < pbounds[\"x\"+str(ii)][0]) or (point_neighbours[\"x\"+str(ii)] > pbounds[\"x\"+str(ii)][1]):\n",
    "            centred_diff = False\n",
    "            forward_diff = True\n",
    "        else:\n",
    "            f_minus = black_box_function(**point_neighbours)\n",
    "        counter += 1\n",
    "\n",
    "        point_neighbours[\"x\"+str(ii)] = point_dict[\"x\"+str(ii)] + learning_rate\n",
    "        if (point_neighbours[\"x\"+str(ii)] < pbounds[\"x\"+str(ii)][0]) or (point_neighbours[\"x\"+str(ii)] > pbounds[\"x\"+str(ii)][1]):\n",
    "            centred_diff = False\n",
    "            backward_diff = True\n",
    "        else:\n",
    "            f_plus = black_box_function(**point_neighbours)\n",
    "        counter += 1\n",
    "        f_plus = black_box_function(**point_dict)\n",
    "\n",
    "        if centred_diff:\n",
    "            grad[\"x\"+str(ii)] = (f_plus - f_minus) / (2.0 * learning_rate)\n",
    "        elif forward_diff:\n",
    "            grad[\"x\"+str(ii)] = (f_plus - f_centre) / learning_rate\n",
    "        elif backward_diff:\n",
    "            grad[\"x\"+str(ii)] = (f_centre - f_minus) / learning_rate\n",
    "        else:\n",
    "            grad[\"x\"+str(ii)] = 0.0\n",
    "            print(\"Broken gradients!\")\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "\n",
    "def grad_descent(start_point, grad, learning_rate, pbounds, num_inputs):\n",
    "    next_point = start_point\n",
    "    for ii in range(num_inputs):\n",
    "        next_point[\"x\"+str(ii)] = start_point[\"x\"+str(ii)] - learning_rate * grad[\"x\"+str(ii)]\n",
    "        if (next_point[\"x\"+str(ii)] < pbounds[\"x\"+str(ii)][0]):\n",
    "            next_point[\"x\"+str(ii)] = pbounds[\"x\"+str(ii)][0]\n",
    "        elif (next_point[\"x\"+str(ii)] > pbounds[\"x\"+str(ii)][1]):\n",
    "            next_point[\"x\"+str(ii)] = pbounds[\"x\"+str(ii)][1]\n",
    "\n",
    "    return next_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de2225-af14-4b2a-9d77-2f51f29b775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_iter = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a90c871-3f77-44cb-a496-e5be04d7a1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mindex = np.argmin(np.sqrt(np.sum(Y_all**2, axis=0)))\n",
    "next_point = {}\n",
    "for ii in range(num_inputs):\n",
    "    next_point[\"x\"+str(ii)] = X_all[ii, mindex]\n",
    "\n",
    "save_points = [None] * n_iter\n",
    "stencil_size = num_inputs * 2 + 1\n",
    "for ieval in range(n_iter):\n",
    "    X_stencil = np.zeros((num_inputs, stencil_size))\n",
    "    \n",
    "    gradient_stencil()\n",
    "    Y_stencil, avg_powers_stencil = run_ifriit_input(stencil_size, X_stencil, run_dir, LMAX, num_nn)\n",
    "    grad = determine_gradient(next_point, learning_rate, pbounds, num_inputs)\n",
    "    \n",
    "    next_point = grad_descent(next_point, grad, learning_rate, pbounds, num_inputs)\n",
    "    Y_new, avg_powers_stencil = run_ifriit_input(1, X_new, run_dir, LMAX, num_nn)\n",
    "    X_all = np.hstack((X_all, X_new))\n",
    "    Y_all = np.hstack((Y_all, Y_new))\n",
    "    avg_powers_all = np.hstack((avg_powers_all, avg_powers_new))\n",
    "    save_points[ieval] = next_point\n",
    "    print(\"Iteration {} value:{}\".format(ieval, black_box_function(**next_point)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79da5299-4665-4ceb-8b66-4b793d3bb257",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 3, Use surrogate NN to pick low RMS from random inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031dec62-d5d3-45c8-9e12-994cacd16898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6aee649-2a79-42ac-bce4-3c08a13163e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 4, Use inverse NN to indentify low rms by inputing other low rms cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e4f0e-8662-4785-b271-26bf3d8c0b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd77d77f-e069-4822-92d9-72e0dd89cf39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 5, Genetic algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b6d4c-1890-4664-9dc9-1e6a1d92ceb1",
   "metadata": {},
   "source": [
    "Iterative procedure taking best features of first generation. Mutate and mix inputs between the best and produce subsequent generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3165c31-f9e0-4ed4-8411-f5cf7cc1e8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0ba0f5b-722d-41ec-889e-3f41d91091c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Method 6, Bayesian optimization (ifriit is high quality source and NN is low quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9609d4-311e-4ee7-a61e-7b4e38c3bd77",
   "metadata": {},
   "source": [
    "Gaussian process surrogate and bayesian optimization used with multiple sources of information. First we create a bayesian model with the true data points and select new simulations based on that. The model (Kriging method?) could use a \"gaussian process approximation\" to reduce computational expense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce61426-7f1a-48f8-9d72-827e7b5c7262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrapper_bayesian_optimizer as wbo\n",
    "\n",
    "init_points = num_examples\n",
    "n_iter = 1000\n",
    "run_dir = \"Data_221102a_symhem_1kex_bo\"\n",
    "iter_dir = \"iter_\"\n",
    "filename_flipped_trainingdata = \"flipped_training_data_and_labels.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b9ec6-f1b1-4bc4-843c-3e1460cd9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {}\n",
    "for ii in range(num_inputs):\n",
    "    pbounds[\"x\"+str(ii)] = (0., 1.)\n",
    "\n",
    "target = -np.sqrt(np.sum(Y_all**2, axis=0)) # Critical to make negative (min not max)\n",
    "print(num_inputs, init_points, np.shape(target))\n",
    "\n",
    "optimizer, utility = wbo.initialize_unknown_func(X_all, target, pbounds, init_points, num_inputs)\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d7a101-3d87-414f-bcef-f8484b05ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "\n",
    "tic = time.perf_counter()\n",
    "for ieval in range(start, n_iter):\n",
    "    next_point = optimizer.suggest(utility)\n",
    "\n",
    "    X_new = np.zeros((num_inputs, 1))\n",
    "    for ii in range(num_inputs):\n",
    "        X_new[ii] = next_point[\"x\"+str(ii)]\n",
    "    \n",
    "    Y_new, avg_powers_new = run_ifriit_input(1, X_new, run_dir, LMAX, num_nn)\n",
    "    \n",
    "    X_all = np.hstack((X_all, X_new))\n",
    "    Y_all = np.hstack((Y_all, Y_new))\n",
    "    avg_powers_all = np.hstack((avg_powers_all, avg_powers_new))\n",
    "    \n",
    "    os.rename(run_dir + \"/run_0\", run_dir + \"/\" + iter_dir + str(ieval))\n",
    "    \n",
    "    #target = black_box_function(**next_point)\n",
    "    target = -np.sqrt(np.sum(Y_new**2))\n",
    "    try:\n",
    "        optimizer.register(params=next_point, target=target)\n",
    "    except:\n",
    "        print(\"Broken input!\", next_point, target)\n",
    "    if (ieval+1)%10 <= 0.0:\n",
    "        toc = time.perf_counter()\n",
    "        print(\"{:0.4f} seconds\".format(toc - tic))\n",
    "        print(str(ieval+1) + \" Bayesian data points added, saving to .nc\")\n",
    "        filename_trainingdata = run_dir + '/' + filename_flipped_trainingdata\n",
    "        nrw.save_training_data(X_all, Y_all, avg_powers_all, filename_trainingdata)\n",
    "        print(optimizer.max)\n",
    "        mindex = np.argmin(np.mean(Y_all, axis=0))\n",
    "        print(mindex)\n",
    "        print(np.sum(Y_all[:,mindex]))\n",
    "        print(np.sqrt(np.sum(Y_all[:,mindex]**2)))\n",
    "        mindex = np.argmin(np.sqrt(np.sum(Y_all**2, axis=0)))\n",
    "        print(mindex)\n",
    "        print(np.sum(Y_all[:,mindex]))\n",
    "        print(np.sqrt(np.sum(Y_all[:,mindex]**2)))\n",
    "print(next_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66f458a-9f89-4425-baaa-4b53d5d70d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_point = optimizer.suggest(utility)\n",
    "print(next_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f542d51-75e7-473b-8e3a-fe162d536ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_extra_point = False\n",
    "ieval = 0\n",
    "\n",
    "if add_extra_point:\n",
    "    sys_params = tdg.define_system_params(run_dir)\n",
    "\n",
    "    # Create new run files\n",
    "    sys_params[\"run_clean\"] = False\n",
    "    nn_params = nng.define_nn_params(num_nn)\n",
    "    X_new, Y_new, avg_powers_all, nn_params = nng.import_training_data_reversed(nn_params, sys_params, LMAX)\n",
    "\n",
    "    os.rename(run_dir + \"/run_0\", run_dir + \"/\" + iter_dir + str(ieval))\n",
    "\n",
    "    target = -np.sqrt(np.sum(Y_new**2))\n",
    "    try:\n",
    "        optimizer.register(params=next_point, target=target)\n",
    "    except:\n",
    "        print(\"Broken input!\", next_point, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bbd1bb-4a1b-412e-855f-aff97b6e40d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(optimizer.res))\n",
    "for i in range(len(optimizer.res)):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, optimizer.res[i]))\n",
    "    if i>5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa0eff7-bd38-4153-8235-788a80642d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimizer.max)\n",
    "print(X_all[:,0], Y_all[:,0])\n",
    "mindex = np.argmin(np.mean(Y_all, axis=0))\n",
    "print(mindex)\n",
    "print(np.sum(Y_all[:,mindex]))\n",
    "print(np.sqrt(np.sum(Y_all[:,mindex]**2)))\n",
    "mindex = np.argmin(np.sqrt(np.sum(Y_all**2, axis=0)))\n",
    "print(mindex)\n",
    "print(np.sum(Y_all[:,mindex]))\n",
    "print(np.sqrt(np.sum(Y_all[:,mindex]**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3dba49-4564-4a4e-8dda-cb8fa35e88db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 7, Grid search algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0393c6d1-d0e5-4743-a7e6-a1bc95c26184",
   "metadata": {},
   "source": [
    "Split the entire search space into a grid (start coarse 2 or 3 cells per dimension) 3^16 = 43M. Evaluate each cell depending on the data points within or 8 nearest neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4101477-d30f-460f-9e81-f25e7d235136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f025763f-d7b4-498b-a1c4-ba2848fff404",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 8, Network search algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b292c18-5aa8-41c0-b3bb-f7a4ed751694",
   "metadata": {},
   "source": [
    "Find a gradient between all data points, use this information to initialize gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37784580-9e53-4183-ad8e-41bf06a96e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df5e1428-8dcf-479d-a30b-2fca0f4db9f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 9, Principle Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3eb066-8f05-4af0-ad97-28b1fed1a6d0",
   "metadata": {},
   "source": [
    "Combine with gradient descent for faster convergence? Enables plotting of dataset in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25c333-f2c6-40bf-8a8f-83d67f1722a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef247b59-ff08-4800-8a8c-4ef9d557e474",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Method 10, Transfer Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e486b0-da4f-4b91-b8dd-1c055f968a88",
   "metadata": {},
   "source": [
    "Generate low quality large dataset (1-50M examples?) using surrogate NN and use this for transfer learning. This might help to evaluate at what stage transfer learning becomes effective (can we use it with a dataset of 1000 or 10000?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00d0b3-c62c-4544-9a26-489690fbe104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
