{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213aa934-414c-4985-8e98-6cccafbe8a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import utils_deck_generation as idg\n",
    "import healpy_pointings as hpoint\n",
    "import netcdf_read_write as nrw\n",
    "import utils_intensity_map as uim\n",
    "from nn_plots import figure_location\n",
    "from scipy.stats import qmc\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "dataset_params = {}\n",
    "sys_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b0786-34fb-4dcd-9df2-f351016e6ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples, size of NN training set\n",
    "dataset_params[\"num_examples\"] = 1\n",
    "sys_params[\"num_processes\"] = 10\n",
    "sys_params[\"num_ex_checkpoint\"] = 1000\n",
    "\n",
    "dataset_params[\"random_seed\"] = 12345\n",
    "rng = np.random.default_rng(dataset_params[\"random_seed\"])\n",
    "\n",
    "num_sim_params = 0\n",
    "# pointings\n",
    "dataset_params[\"surface_cover_radians\"] = np.radians(45.0)\n",
    "num_sim_params += 2\n",
    "# defocus\n",
    "dataset_params[\"defocus_range\"] = 20.0 # mm\n",
    "num_sim_params += 1\n",
    "#power\n",
    "dataset_params[\"min_power\"] = 0.5 # fraction of full power\n",
    "num_sim_params += 1\n",
    "dataset_params[\"num_sim_params\"] = num_sim_params\n",
    "\n",
    "\n",
    "dataset_params[\"imap_nside\"] = 256\n",
    "sys_params[\"run_gen_deck\"] = True\n",
    "sys_params[\"run_sims\"] = True\n",
    "sys_params[\"run_compression\"] = True\n",
    "sys_params[\"run_clean\"] = False\n",
    "\n",
    "dataset_params[\"run_type\"] = \"nif\" #\"test\" #\"nif\"\n",
    "run_data = idg.import_nif_config()\n",
    "\n",
    "sys_params[\"root_dir\"] = \"Data\"\n",
    "sys_params[\"sim_dir\"] = \"run_\"\n",
    "sys_params[\"trainingdata_filename\"] = \"training_data_and_labels.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d9e704-43d6-4aa4-a710-a942d3c31799",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params[\"LMAX\"] = 30\n",
    "dataset_params[\"num_coeff\"] = int(((dataset_params[\"LMAX\"] + 2) * (dataset_params[\"LMAX\"] + 1))/2.0)\n",
    "# Assume symmetry\n",
    "dataset_params[\"num_output\"] = int(run_data['num_cones']/2) * dataset_params[\"num_sim_params\"]\n",
    "print(dataset_params[\"num_coeff\"]*2, dataset_params[\"num_examples\"], dataset_params[\"num_output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2a3408-6a59-4b28-bf5b-8212f4b7142c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generate and Save Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d03df0f-9bd9-45d4-ab9c-14fc97858ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = qmc.LatinHypercube(d=dataset_params[\"num_output\"], seed=rng)\n",
    "sample = sampler.random(n=dataset_params[\"num_examples\"])\n",
    "Y_train = sample.T\n",
    "print(np.shape(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86466b15-2238-470c-9728-332fbe8a18e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_params = idg.create_run_files(Y_train, dataset_params, sys_params, run_data)\n",
    "\n",
    "X_train = np.zeros((dataset_params[\"num_coeff\"] * 2, dataset_params[\"num_examples\"]))\n",
    "avg_powers = np.zeros(dataset_params[\"num_examples\"])\n",
    "\n",
    "min_parallel = 0\n",
    "max_parallel = -1\n",
    "run_location = sys_params[\"root_dir\"] + \"/\" + sys_params[\"sim_dir\"]\n",
    "chkp_marker = 1.0\n",
    "\n",
    "if sys_params[\"run_sims\"]:\n",
    "    num_parallel_runs = int(dataset_params[\"num_examples\"] / sys_params[\"num_processes\"])\n",
    "    if num_parallel_runs > 0:\n",
    "        for ir in range(num_parallel_runs):\n",
    "            min_parallel = ir * sys_params[\"num_processes\"]\n",
    "            max_parallel = (ir + 1) * sys_params[\"num_processes\"] - 1\n",
    "            for iex in range(min_parallel, max_parallel+1):\n",
    "                idg.copy_ifriit_exc(run_location, iex)\n",
    "            subprocess.check_call([\"./bash_parallel_ifriit\", run_location, str(min_parallel), str(max_parallel)])\n",
    "            for iex in range(min_parallel, max_parallel+1):\n",
    "                X_train1, avg_power1 = nrw.retrieve_xtrain_and_delete(run_location, run_data['Beam'], iex, dataset_params, sys_params)\n",
    "\n",
    "                X_train[:,iex] = X_train1\n",
    "                avg_powers[iex] = avg_power1\n",
    "            if sys_params[\"run_compression\"]:\n",
    "                if ((max_parallel + 1) >= (chkp_marker * sys_params[\"num_ex_checkpoint\"])):\n",
    "                    print(\"Save training data checkpoint at run: \" + str(max_parallel))\n",
    "                    nrw.save_training_data(X_train[:,:max_parallel+1], Y_train[:,:max_parallel+1], avg_powers[:max_parallel+1], filename_trainingdata)\n",
    "                    chkp_marker +=1\n",
    "\n",
    "    if max_parallel != (dataset_params[\"num_examples\"] - 1):\n",
    "        min_parallel = max_parallel + 1\n",
    "        max_parallel = dataset_params[\"num_examples\"] - 1\n",
    "        for iex in range(min_parallel, max_parallel+1):\n",
    "            idg.copy_ifriit_exc(run_location, iex)\n",
    "        subprocess.check_call([\"./bash_parallel_ifriit\", run_location, str(min_parallel), str(max_parallel)])\n",
    "        for iex in range(min_parallel, max_parallel+1):\n",
    "            X_train1, avg_power1 = nrw.retrieve_xtrain_and_delete(run_location, run_data['Beam'], iex, dataset_params, sys_params)\n",
    "\n",
    "            X_train[:,iex] = X_train1\n",
    "            avg_powers[iex] = avg_power1\n",
    "\n",
    "if sys_params[\"run_compression\"]:\n",
    "    filename_trainingdata = sys_params[\"root_dir\"] + \"/\" + sys_params[\"trainingdata_filename\"]\n",
    "    nrw.save_training_data(X_train, Y_train, avg_powers, filename_trainingdata)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1638f51b-5ee7-41e1-b4e6-709e6d6a8353",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Diagnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c815b9a9-0b10-4ab1-b8c9-a5883639828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import healpy as hp\n",
    "import utils_intensity_map as uim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257f48cf-98dd-47d2-9ff4-85ac31888714",
   "metadata": {},
   "outputs": [],
   "source": [
    "iex = 0\n",
    "\n",
    "run_location = sys_params[\"root_dir\"] + \"/\" + sys_params[\"sim_dir\"] + str(iex)\n",
    "theta_slice = slice(0,29,4)\n",
    "phi_slice   = slice(1,30,4)\n",
    "power_slice = slice(3,32,4)\n",
    "\n",
    "cone_theta_offset = dataset_params[\"sim_params\"][theta_slice,iex]\n",
    "cone_phi_offset = dataset_params[\"sim_params\"][phi_slice,iex]\n",
    "cone_powers = dataset_params[\"sim_params\"][power_slice,iex]\n",
    "\n",
    "beams_prev = 0\n",
    "beams_tot = 0\n",
    "total_power = 0\n",
    "for icone in range(run_data['num_cones']):\n",
    "    beams_per_cone = run_data['beams_per_cone'][icone]\n",
    "    beams_tot += beams_per_cone\n",
    "    total_power += cone_powers[icone] * beams_per_cone\n",
    "    beams_prev += beams_per_cone\n",
    "mean_power_fraction = total_power / run_data['nbeams']\n",
    "\n",
    "intensity_map = nrw.read_intensity(run_location, dataset_params[\"imap_nside\"])\n",
    "intensity_map_rms_spatial = uim.readout_intensity(run_data, intensity_map, mean_power_fraction)\n",
    "\n",
    "hp.mollview(intensity_map,unit=r\"$\\rm{W/cm^2}$\",flip=\"geo\")\n",
    "hp.graticule()\n",
    "port_theta = run_data[\"Port_centre_theta\"]\n",
    "port_phi = run_data[\"Port_centre_phi\"]\n",
    "hp.projscatter(port_theta, port_phi)\n",
    "hp.projscatter(dataset_params[\"theta_pointings\"][:,iex], dataset_params[\"phi_pointings\"][:,iex])\n",
    "plt.savefig(figure_location+'/intensity_mollweide.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ae723c-a865-4f85-b9cf-700a9579ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.mollview(intensity_map_rms_spatial, unit=\"Deviation from mean (%)\",flip=\"geo\")\n",
    "plt.savefig(figure_location+'/deviation_from_mean_mollweide.png', dpi=300, bbox_inches='tight')\n",
    "hp.graticule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c23e0-5a36-4d67-babd-62644b2fc54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LMAX = dataset_params[\"LMAX\"]\n",
    "power_spectrum_unweighted, power_spectrum_weighted = uim.power_spectrum(intensity_map, LMAX)\n",
    "x_max = 20\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "plt.plot(np.arange(LMAX), power_spectrum_unweighted * 100.0)\n",
    "ax.set_xticks(range(0, LMAX+1, int(LMAX/5)))\n",
    "plt.xlim([0, x_max])\n",
    "plt.title(\"Unweighted Modes LLE\")\n",
    "plt.xlabel(\"l mode\")\n",
    "plt.ylabel(r\"rms amplitude ($\\%$)\")\n",
    "plt.savefig(figure_location+\"/unweighted_modes.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "plt.plot(np.arange(LMAX), power_spectrum_weighted * 100.0)\n",
    "ax.set_xticks(range(0, LMAX+1, int(LMAX/5)))\n",
    "plt.xlim([0, x_max])\n",
    "plt.title(\"Weighted Modes\")\n",
    "plt.xlabel(\"l mode\")\n",
    "plt.ylabel(r\"rms amplitude ($\\%$)\")\n",
    "plt.savefig(figure_location+\"/weighted_modes.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a4c119-b464-4f3a-a528-dd4b8c9155cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_map_normalized, avg_power = uim.imap_norm(intensity_map)\n",
    "hp.mollview(intensity_map_normalized, unit=\"Deviation from Mean\", flip=\"geo\")\n",
    "\n",
    "X_train = uim.imap2xtrain(intensity_map_normalized, LMAX, avg_power)\n",
    "intensity_map_normalized2 = uim.xtrain2imap(X_train, LMAX, dataset_params[\"imap_nside\"], avg_power)\n",
    "hp.mollview(intensity_map_normalized2, unit=\"Deviation from Mean\",flip=\"geo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8364953-8f8f-4b68-9f90-a32ecd5de19c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca18ad-2050-4a3c-9c6c-924a951feb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "from os import path\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f61a17-aa37-47bc-8e99-3262fdc70b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = \"Data1\"\n",
    "dataset2 = \"Data2\"\n",
    "\n",
    "new_file = \"Data3\"\n",
    "\n",
    "filename_trainingdata = dataset1 + \"/\" + sys_params[\"trainingdata_filename\"] \n",
    "training_data = Dataset(filename_trainingdata)\n",
    "X_1 = training_data.variables[\"X_train\"][:]\n",
    "Y_1 = training_data.variables[\"Y_train\"][:]\n",
    "avg_powers_1 = training_data.variables[\"avg_powers\"][:]\n",
    "training_data.close()\n",
    "\n",
    "filename_trainingdata = dataset2 + \"/\" + sys_params[\"trainingdata_filename\"] \n",
    "training_data = Dataset(filename_trainingdata)\n",
    "X_2 = training_data.variables[\"X_train\"][:]\n",
    "Y_2 = training_data.variables[\"Y_train\"][:]\n",
    "avg_powers_2 = training_data.variables[\"avg_powers\"][:]\n",
    "training_data.close()\n",
    "\n",
    "X_train = np.hstack((X_1, X_2))\n",
    "Y_train = np.hstack((Y_1, Y_2))\n",
    "avg_powers = np.hstack((avg_powers_1, avg_powers_2))\n",
    "print(np.shape(X_train), np.shape(Y_train), np.shape(avg_powers))\n",
    "\n",
    "filename_trainingdata = new_file + \"/\" + sys_params[\"trainingdata_filename\"] \n",
    "nrw.save_training_data(X_train, Y_train, avg_powers, filename_trainingdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a70c8f3-ae76-4bc6-a5da-5f1bb91c2aa2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create data structure for data set and label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6350add7-34b4-430c-bc15-07ee7a5a0c08",
   "metadata": {},
   "source": [
    "## Load Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daadcae2-0226-46f4-bc28-1b9fa842b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "from os import path\n",
    "import os\n",
    "import healpy as hp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db60045-b05d-483c-b27c-bab2e197da72",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_trainingdata = sys_params[\"root_dir\"] + \"/\" + sys_params[\"trainingdata_filename\"]\n",
    "training_data = Dataset(filename_trainingdata)\n",
    "X_all = training_data.variables[\"X_train\"][:]\n",
    "Y_all = training_data.variables[\"Y_train\"][:]\n",
    "avg_powers_all = training_data.variables[\"avg_powers\"][:]\n",
    "training_data.close()\n",
    "\n",
    "print(np.shape(X_all), np.shape(Y_all))\n",
    "num_examples = np.shape(X_all)[1]\n",
    "input_size = np.shape(X_all)[0]\n",
    "output_size = np.shape(Y_all)[0]\n",
    "\n",
    "run_shuffle = False\n",
    "random_seed = 12345\n",
    "rng = np.random.default_rng(random_seed)\n",
    "\n",
    "test_size = int(num_examples / 100)\n",
    "if test_size == 0:\n",
    "    test_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd89f28-6f87-4084-bb83-9a703e188dc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Shuffle and Seperate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8bdee7-5edd-42db-ad3a-be8739ca30d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_shuffle:\n",
    "    index_shuf = list(range(num_examples))\n",
    "    rng.shuffle(index_shuf)\n",
    "    index_shuf = np.array(index_shuf)\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    train_avg_powers = []\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    test_avg_powers = []\n",
    "    j=0\n",
    "    for i in index_shuf:\n",
    "        if (j < test_size):\n",
    "            X_test.append(X_all[:,i])\n",
    "            Y_test.append(Y_all[:,i])\n",
    "            test_avg_powers.append(avg_powers_all[i])\n",
    "        else:\n",
    "            X_train.append(X_all[:,i])\n",
    "            Y_train.append(Y_all[:,i])\n",
    "            train_avg_powers.append(avg_powers_all[i])\n",
    "        j = j + 1\n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "    X_train = np.array(X_train)\n",
    "    Y_train = np.array(Y_train)\n",
    "else:\n",
    "    X_test = X_all.T[:test_size,:]\n",
    "    Y_test = Y_all.T[:test_size,:]\n",
    "    test_avg_powers = avg_powers_all[:test_size]\n",
    "\n",
    "    X_train = X_all.T[test_size:,:]\n",
    "    Y_train = Y_all.T[test_size:,:]\n",
    "    train_avg_powers = avg_powers_all[test_size:]\n",
    "\n",
    "print(np.shape(X_train), np.shape(Y_train), np.shape(X_test), np.shape(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c82c60-4c93-43c6-b402-c754f7868daa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Check Shuffle Maintained labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86ef8d3-6354-4d08-b208-d8e48805640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_shuffle:\n",
    "    ind = 0\n",
    "    ind_first = np.where(index_shuf == ind)\n",
    "    ind_first = np.squeeze(ind_first[0])\n",
    "\n",
    "    print(Y_all[:,ind])\n",
    "    intensity_map_normalized2 = uim.xtrain2imap(X_all[:,ind], dataset_params[\"LMAX\"], dataset_params[\"imap_nside\"], avg_powers_all[ind])\n",
    "    hp.mollview(intensity_map_normalized2, unit=\"Deviation from Mean\",flip=\"geo\")\n",
    "\n",
    "    if ind_first >= test_size:\n",
    "        i = ind_first-test_size\n",
    "        print(Y_train[i,:])\n",
    "        cdata = X_train[i,:]\n",
    "        apwrs = train_avg_powers[i]\n",
    "    else:\n",
    "        i = ind_first\n",
    "        print(Y_test[i,:])\n",
    "        cdata = X_test[i,:]\n",
    "        apwrs = test_avg_powers[i]\n",
    "    intensity_map_normalized2 = uim.xtrain2imap(cdata, dataset_params[\"LMAX\"], dataset_params[\"imap_nside\"], apwrs)\n",
    "    hp.mollview(intensity_map_normalized2, unit=\"Deviation from Mean\",flip=\"geo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33fa8c7-5159-47ec-bc46-7850318eb5a3",
   "metadata": {},
   "source": [
    "## Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0d0ea-d714-4f66-8176-b3a1b3c61804",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.mean(X_train)\n",
    "sigma = np.std(X_train)\n",
    "\n",
    "X_train = (X_train - mu) / sigma\n",
    "X_test = (X_test - mu) / sigma\n",
    "\n",
    "print(np.std(X_train), np.std(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb279fd2-8b48-4aa9-aed9-4c3b26e8e6b8",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a12d90c-894b-47b5-b0d9-3eef77122438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_neural_network as tfnn\n",
    "import matplotlib.pyplot as plt\n",
    "import nn_plots as nnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00bf58-ed71-4da6-bcb4-f4a2bd788bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_nn_weights = \"neural_network_weights\"\n",
    "\n",
    "lr = 0.001\n",
    "num_epochs = 100\n",
    "costs = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "epochs = []\n",
    "hidden_units1 = 25\n",
    "hidden_units2 = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065fe768-98be-410a-885b-a8c24d68fede",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs, train_acc, test_acc, epochs = tfnn.model_wrapper(X_train, Y_train, X_test, Y_test, learning_rate = lr, num_epochs = num_epochs, hidden_units1=hidden_units1, hidden_units2=hidden_units2)\n",
    "filename_nn_weights = dir_nn_weights + \"/NN\" + str(0)\n",
    "nrw.save_nn_weights(parameters, filename_nn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412a7835-209f-46a9-bb26-44b8dd7e3d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnp.plotting(epochs, costs, train_acc, test_acc, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e8ebc0-135e-4a3f-9b9c-bd85e88d5717",
   "metadata": {},
   "source": [
    "## Open NN Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2dfe5e-e3e5-4b00-a6cc-98760f1a79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = nrw.read_nn_weights(filename_nn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c58f39-073a-4d71-bad6-161cbc0dd829",
   "metadata": {},
   "source": [
    "## Restart Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dfb5bd-89b5-4232-80ce-d323f2690f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = num_epochs\n",
    "num_epochs = 1000\n",
    "\n",
    "parameters, costs2, train_acc2, test_acc2, epochs2 = tfnn.model_wrapper(X_train, Y_train, X_test, Y_test, learning_rate = lr, num_epochs = num_epochs, start_epoch = start_epoch, nn_weights = parameters)\n",
    "filename_nn_weights = dir_nn_weights + \"/NN\" + str(0)\n",
    "nrw.save_nn_weights(parameters, filename_nn_weights)\n",
    "\n",
    "costs = np.append(costs, costs2)\n",
    "train_acc = np.append(train_acc, train_acc2)\n",
    "test_acc = np.append(test_acc, test_acc2)\n",
    "epochs = np.append(epochs, epochs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446f24be-a0a8-4988-9bcb-bd1895354639",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnp.plotting(epochs, costs, train_acc, test_acc, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265eb6e1-9afc-40e0-8b93-86ad460452bb",
   "metadata": {},
   "source": [
    "## Apply NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bdd7e5-249b-4b59-88cf-c79b620485e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = 0\n",
    "\n",
    "x_test = np.reshape(X_test[ie,:], (-1,input_size))\n",
    "y_true = np.squeeze(Y_test[ie,:])\n",
    "\n",
    "y_pred = tfnn.apply_network(x_test, parameters)\n",
    "y_pred = np.squeeze(y_pred)\n",
    "print(y_pred)\n",
    "print(y_true)\n",
    "print((np.abs(y_true - y_pred)) / y_true * 100)\n",
    "print(np.mean((np.abs(y_true - y_pred)) / y_true * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbdc059-6cf0-4998-bfca-ec665674e071",
   "metadata": {},
   "source": [
    "## Training Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ce6747-e1b5-4088-a58e-e17903e0c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nn = 3\n",
    "num_dumps = int(num_epochs / 10) + 1\n",
    "\n",
    "costs = np.zeros((num_dumps, num_nn))\n",
    "train_acc = np.zeros((num_dumps, num_nn))\n",
    "test_acc = np.zeros((num_dumps, num_nn))\n",
    "epochs = np.zeros((num_dumps, num_nn))\n",
    "for inn in range(num_nn):\n",
    "    parameters, costs1, train_acc1, test_acc1, epochs1 = tfnn.model_wrapper(X_train, Y_train, X_test, Y_test, learning_rate = lr, num_epochs = num_epochs, hidden_units1=hidden_units1, hidden_units2=hidden_units2, initialize_seed=inn, print_cost = False)\n",
    "    filename_nn_weights = dir_nn_weights + \"/NN\" + str(inn)\n",
    "    nrw.save_nn_weights(parameters, filename_nn_weights)\n",
    "    costs[:,inn] = costs1\n",
    "    train_acc[:,inn] = train_acc1\n",
    "    test_acc[:,inn] = test_acc1\n",
    "    epochs[:,inn] = epochs1\n",
    "    print(\"Trained neural network index/seed: \", inn)\n",
    "nnp.plotting(epochs, costs, train_acc, test_acc, lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
